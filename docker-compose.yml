version: '3.8'

services:
  # ---------------------------
  # 1. 向量資料庫 (Qdrant)
  # ---------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant-server
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_storage:/qdrant/storage

  # ---------------------------
  # 2. LLM 推論引擎 (vLLM)
  # ---------------------------
  llm-engine:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    runtime: nvidia # 關鍵：使用 GPU
    restart: always
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN} # 從 .env 讀取
    # 針對您的 Breeze2-8B 設定
    command: >
      --model MediaTek-Research/Llama-Breeze2-8B-Instruct-v0_1
      --served-model-name breeze-8b
      --dtype bfloat16
      --api-key EMPTY
      --max-model-len 8192
      --gpu-memory-utilization 0.85 
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ---------------------------
  # 3. 您的 API (FastAPI + Embedding)
  # ---------------------------
  rag-api:
    build: .
    container_name: rag-api-server
    restart: always
    ports:
      - "5000:5000"
    depends_on:
      - qdrant
      - llm-engine
    # 這裡覆蓋 config.py 的預設值
    environment:
      - APP_ENV=production
      - QDRANT_HOST=qdrant        # 直接寫 service name
      - QDRANT_PORT=6333
      - LLM_API_HOST=http://llm-engine # 直接寫 service name
      - LLM_API_PORT=8000
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN} # 從 .env 讀取
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./data:/app/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
